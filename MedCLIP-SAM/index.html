<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation">
  <meta name="keywords"
        content="MedCLIP-SAM, BiomedCLIP, CLIP, SAM, Zero-shot Segmentation, Weakly Supervised Segmentation, Medical Imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MedCLIP-SAM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>

<body>

<!-- ================= HERO ================= -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation
      </h1>

      <div class="is-size-5 publication-authors">
        Taha Koleilat · Hojat Asgariandehkordi · Hassan Rivaz · Yiming Xiao
      </div>

      <div class="is-size-6 publication-authors">
        Concordia University · Health-X Lab · IMPACT Lab
      </div>

      <div class="buttons is-centered mt-4">
        <a class="button is-dark is-rounded" href="https://arxiv.org/abs/2403.20253" target="_blank">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>

        <a class="button is-dark is-rounded"
           href="https://github.com/HealthX-Lab/MedCLIP-SAM"
           target="_blank">
          <span class="icon">
            <img src="./static/icons/github.webp" alt="GitHub" style="height:18px;">
          </span>
          <span>Code</span>
        </a>

        <a class="button is-dark is-rounded" href="https://drive.google.com/file/d/1uYtyg3rClE-XXPNuEz7s6gYq2p48Z08p/view?usp=sharing">
          <span class="icon">
            <img src="./static/icons/datasets.png" alt="Datasets" style="height:18px;">
          </span>
          <span>Datasets</span>
        </a>

        <a class="button is-dark is-rounded" href="#colab-demo">
          <span class="icon">
            <img src="./static/icons/colab.png" alt="Demo" style="height:18px;">
          </span>
          <span>Demo</span>
        </a>

        <a class="button is-dark is-rounded" href="#BibTeX">
          <span class="icon">
            <img src="./static/icons/bibtex.png" alt="BibTeX" style="height:18px;">
          </span>
          <span>BibTeX</span>
        </a>
      </div>

      <p class="is-size-7 has-text-grey mt-2">
        Published at <i>MICCAI 2024</i>.
      </p>
    </div>
  </div>
</section>

<!-- ================= TEASER ================= -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">

      <video autoplay muted loop playsinline
             style="max-width:100%; border-radius:8px;">
        <source src="./static/videos/demo.mp4" type="video/mp4">
        <!-- Fallback GIF -->
        <img src="./static/videos/demo.gif"
             alt="MedCLIP-SAM text-driven segmentation demo">
      </video>

      <h2 class="subtitle mt-4">
        MedCLIP-SAM enables interactive, text-driven medical image segmentation
        by combining BiomedCLIP saliency, CRF refinement, and SAM-based mask
        generation in zero-shot and weakly supervised settings.
      </h2>

    </div>
  </div>
</section>

<!-- ================= ABSTRACT ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Medical image segmentation of anatomical structures and pathology is
        crucial in modern clinical diagnosis, disease study, and treatment
        planning. While deep learning-based methods have achieved strong
        performance, they often lack data efficiency, generalizability, and
        interactability. In this work, we propose <b>MedCLIP-SAM</b>, a novel
        framework that bridges vision–language models and segmentation
        foundation models to enable <b>text-driven universal medical image
        segmentation</b>. MedCLIP-SAM integrates BiomedCLIP fine-tuned with a
        <b>Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE)</b>
        loss, <b>gScoreCAM</b>-based saliency generation, CRF post-processing,
        and <b>Segment Anything Model (SAM)</b> refinement. The framework
        supports both <b>zero-shot</b> and <b>weakly supervised</b> segmentation
        and is validated across breast ultrasound, brain MRI, and chest X-ray
        datasets, demonstrating strong accuracy and generalization.
      </p>
    </div>
  </div>
</section>

<!-- ================= OVERVIEW ================= -->
<section class="section is-light" id="overview">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Overview</h2>

    <div class="content has-text-justified">
      <ul>
        <li><b>DHN-NCE Fine-tuning:</b> Efficiently adapts BiomedCLIP to medical images using hard negatives and decoupled contrastive learning.</li>
        <li><b>Text-driven Saliency:</b> gScoreCAM generates class-relevant saliency maps from text prompts.</li>
        <li><b>CRF Post-processing:</b> Refines saliency maps into coarse pseudo-masks.</li>
        <li><b>SAM Refinement:</b> Uses box prompts derived from saliency to produce high-quality segmentation masks.</li>
        <li><b>Weak Supervision:</b> Zero-shot masks can optionally supervise downstream segmentation networks.</li>
      </ul>
    </div>

    <figure class="image">
      <img src="./static/images/framework.jpg" alt="MedCLIP-SAM framework">
      <figcaption class="has-text-centered is-size-7 mt-2">
        Overview of the MedCLIP-SAM framework.
      </figcaption>
    </figure>
  </div>
</section>

<!-- ================= RESULTS ================= -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Results</h2>

    <div class="content has-text-justified">
      <p>
        We present representative qualitative results of <b>MedCLIP-SAM</b>
        across three medical imaging modalities: breast ultrasound, brain MRI,
        and chest X-ray. The results highlight the effectiveness of text-driven
        prompts in guiding segmentation without requiring pixel-level
        annotations, while SAM refinement improves boundary adherence and
        anatomical consistency.
      </p>
    </div>

    <figure class="image">
      <img src="./static/images/samples.png" alt="MedCLIP-SAM qualitative results">
      <figcaption class="has-text-centered is-size-7 mt-2">
        Qualitative comparison of segmentation results across modalities.
        From left to right: input image, CLIP-based saliency, zero-shot SAM
        segmentation, weakly supervised refinement, and ground truth.
      </figcaption>
    </figure>
  </div>
</section>

<!-- ================= SALIENCY COMPARISON ================= -->
<section class="section" id="saliency-comparison">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Saliency Map Analysis</h2>

    <div class="content has-text-justified">
      <p>
        We analyze the quality of text-driven saliency maps produced by
        BiomedCLIP using <b>gScoreCAM</b>. Compared to conventional CAM
        techniques, gScoreCAM provides more localized and semantically aligned
        responses, enabling reliable bounding-box prompt generation for SAM
        across diverse imaging modalities.
      </p>
    </div>
  </div>
</section>

<!-- ================= DATASETS ================= -->
<section class="section is-light" id="datasets">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Datasets</h2>
    <div class="content has-text-justified">
      <ul>
        <li><b>Breast Ultrasound:</b> BUSI + UDIAT</li>
        <li><b>Brain MRI:</b> Brain Tumor Dataset</li>
        <li><b>Chest X-ray:</b> COVID-QU-Ex</li>
      </ul>
    </div>
  </div>
</section>

<!-- ================= DEMO ================= -->
<section class="section" id="colab-demo">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Colab Demo</h2>
    <a class="button is-dark is-rounded"
       href="https://colab.research.google.com/drive/143KK2JrMH-AKhMZq1fDt_8om-183POAh"
       target="_blank">
      <span class="icon"><img src="./static/icons/colab.png" style="height:18px;"></span>
      <span>Open in Colab</span>
    </a>
  </div>
</section>

<!-- ================= BIBTEX ================= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@inproceedings{koleilat2024medclip,
  title={MedCLIP-SAM: Bridging text and image towards universal medical image segmentation},
  author={Koleilat, Taha and Asgariandehkordi, Hojat and Rivaz, Hassan and Xiao, Yiming},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={643--653},
  year={2024},
  organization={Springer}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Website adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
    </p>
  </div>
</footer>

</body>
</html>
