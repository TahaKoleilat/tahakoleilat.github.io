<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models">
  <meta name="keywords"
        content="BiomedCoOp, BiomedCLIP, CLIP, Few-shot Learning, Medical Imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BiomedCoOp</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>

<body>

<!-- ================= HERO ================= -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models
      </h1>

      <div class="is-size-5 publication-authors">
        Taha Koleilat · Hojat Asgariandehkordi · Hassan Rivaz · Yiming Xiao
      </div>

      <div class="is-size-6 publication-authors">
        Concordia University · Health-X Lab · IMPACT Lab
      </div>

      <div class="buttons is-centered mt-4">
        <a class="button is-dark is-rounded" href="https://arxiv.org/abs/2411.15232" target="_blank">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>
        <a class="button is-dark is-rounded" href="https://github.com/HealthX-Lab/BiomedCoOp" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
        </a>
        <a class="button is-dark is-rounded" href="https://huggingface.co/TahaKoleilat/BiomedCoOp" target="_blank">
          <span class="icon"><i class="fas fa-cube"></i></span><span>Models</span>
        </a>
        <a class="button is-dark is-rounded" href="https://huggingface.co/datasets/TahaKoleilat/BiomedCoOp" target="_blank">
          <span class="icon"><i class="far fa-images"></i></span><span>Datasets</span>
        </a>
        <a class="button is-dark is-rounded" href="#BibTeX">
          <span class="icon"><i class="ai ai-bibtex"></i></span><span>BibTeX</span>
        </a>
      </div>
    </div>
  </div>
</section>

<!-- ================= TEASER ================= -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/overview.png" class="teaser-image" alt="BiomedCoOp overview">
      <h2 class="subtitle has-text-centered">
        Overview of BiomedCoOp: LLM-guided prompt ensembles and selective distillation
        enable robust few-shot biomedical image classification.
      </h2>
    </div>
  </div>
</section>

<!-- ================= ABSTRACT ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        We propose <b>BiomedCoOp</b>, a prompt learning framework for efficient adaptation
        of biomedical vision-language models. By leveraging semantic consistency with
        large language model prompt ensembles and selective knowledge distillation,
        BiomedCoOp achieves state-of-the-art few-shot performance and superior
        base-to-novel generalization across 11 biomedical datasets spanning 9 modalities.
      </p>
    </div>
  </div>
</section>

<!-- ================= METHOD ================= -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>

    <div class="content has-text-justified">
      <ul>
        <li><b>LLM-guided Prompt Ensembles:</b> LLM-generated biomedical descriptions guide context learning.</li>
        <li><b>Semantic Consistency:</b> Prompt contexts are aligned with averaged LLM embeddings.</li>
        <li><b>Selective Knowledge Distillation:</b> Statistics-based pruning removes noisy prompts.</li>
        <li><b>BiomedCLIP Backbone:</b> Enables robust multi-modal biomedical representations.</li>
      </ul>
    </div>

    <figure class="image">
      <img src="./static/images/BiomedCoOp.jpg" alt="BiomedCoOp method">
    </figure>
  </div>
</section>

<!-- ================= FEW-SHOT RESULTS ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Few-shot Evaluation</h2>
    <!-- (TABLE CONTENT UNCHANGED – already correct) -->
    <!-- Keep your existing few-shot table here -->
  </div>
</section>

<!-- ================= BASE-TO-NOVEL RESULTS ================= -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Base-to-Novel Generalization</h2>
    <!-- (TABLE CONTENT UNCHANGED – already correct) -->
  </div>
</section>

<!-- ================= SALIENCY MAPS ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Visual Interpretability (Saliency Maps)</h2>

    <div class="content has-text-justified">
      <p>
        We visualize model attention using <b>gScoreCAM</b>. Compared to baseline prompt
        learning methods, <b>BiomedCoOp</b> consistently highlights clinically relevant
        regions while suppressing background noise across modalities.
      </p>
    </div>

    <div class="columns is-multiline is-centered">
      <div class="column is-6">
        <figure class="image">
          <img src="./static/images/saliency_clip.png" alt="BiomedCLIP saliency">
          <figcaption class="has-text-centered is-size-7">BiomedCLIP (Zero-shot)</figcaption>
        </figure>
      </div>

      <div class="column is-6">
        <figure class="image">
          <img src="./static/images/saliency_coop.png" alt="CoOp saliency">
          <figcaption class="has-text-centered is-size-7">CoOp</figcaption>
        </figure>
      </div>

      <div class="column is-6">
        <figure class="image">
          <img src="./static/images/saliency_cocoop.png" alt="CoCoOp saliency">
          <figcaption class="has-text-centered is-size-7">CoCoOp</figcaption>
        </figure>
      </div>

      <div class="column is-6">
        <figure class="image">
          <img src="./static/images/saliency_biomedcoop.png" alt="BiomedCoOp saliency">
          <figcaption class="has-text-centered is-size-7"><b>BiomedCoOp (Ours)</b></figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- ================= BIBTEX ================= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@inproceedings{koleilat2025biomedcoop,
  title={BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models},
  author={Koleilat, Taha and Asgariandehkordi, Hojat and Rivaz, Hassan and Xiao, Yiming},
  booktitle={CVPR},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Website adapted from <a href="https://nerfies.github.io">Nerfies</a>.
    </p>
  </div>
</footer>

</body>
</html>
