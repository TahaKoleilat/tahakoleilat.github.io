<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Taha Koleilat</title> <meta name="author" content="Taha Koleilat"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/%EF%B8%8F/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://tahakoleilat.github.io//"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%74%61%68%61%6B%6F%6C%65%69%6C%61%74@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=chB2OjUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/TahaKoleilat" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/tahakoleilat" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Taha</span> Koleilat </h1> <p class="desc">PhD Candidate, <a href="https://www.concordia.ca/" rel="external nofollow noopener" target="_blank">Concordia University</a>, Canada · BEng from <a href="https://www.aub.edu.lb/" rel="external nofollow noopener" target="_blank">American University of Beirut</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_img-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_img-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_img-1400.webp"></source> <img src="/assets/img/profile_img.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile_img.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Montreal, Canada</p> </div> </div> <div class="clearfix"> <p>Hi, I am <strong>Taha Koleilat</strong>, a PhD candidate in <strong>Electrical &amp; Computer Engineering</strong> at <strong>Concordia University</strong>, Montreal. I am a Graduate Research Assistant at the <strong>IMPACT</strong> and <strong>Health-X Labs</strong>, supervised by <a href="https://www.concordia.ca/ginacody/electrical-computer-engineering/faculty/hassan-rivaz.html" rel="external nofollow noopener" target="_blank">Prof. Hassan Rivaz</a> and <a href="https://www.concordia.ca/ginacody/computer-science-software-engineering/faculty/yiming-xiao.html" rel="external nofollow noopener" target="_blank">Prof. Yiming Xiao</a>.</p> <p>Previously, I attained my <strong>BEng in Computer &amp; Communications Engineering</strong> at the <strong>American University of Beirut</strong>, graduating with <strong>High Distinction (4.0/4.0)</strong> and a minor in <strong>Economics</strong>.</p> <p>My research lies at the intersection of <strong>medical image analysis</strong>, <strong>vision–language models</strong>, and <strong>foundation models</strong>. I focus on developing <strong>data-efficient, generalizable, and interpretable multimodal learning frameworks</strong> that bridge <strong>medical images and clinical text</strong>, enabling robust performance under <strong>low-data and cross-domain settings</strong>.</p> <p>My work has been published at leading venues including <strong>CVPR</strong>, <strong>MICCAI</strong>, and <strong>Medical Image Analysis</strong>, and has been supported by major competitive awards such as the <strong>FRQNT Doctoral Scholarship</strong>.</p> <p><b>Currently, I am</b> working on <strong>text-driven medical image segmentation</strong>, <strong>vision–language model adaptation</strong>, and <strong>uncertainty-aware multimodal learning</strong> for efficient and reliable clinical deployment.</p> <p>I am serving as a reviewer for leading venues including <strong>IEEE TPAMI</strong>, <strong>MICCAI</strong>, <strong>Medical Image Analysis (MedIA)</strong>, and <strong>IEEE TMI</strong>.</p> <p><a href="mailto:tahakoleilat@gmail.com">Email</a> / <a href="https://scholar.google.ca/citations?user=chB2OjUAAAAJ&amp;hl=en&amp;authuser=1" rel="external nofollow noopener" target="_blank">Google Scholar</a> / <a href="https://github.com/TahaKoleilat" rel="external nofollow noopener" target="_blank">GitHub</a> / <a href="https://www.linkedin.com/in/tahakoleilat/" rel="external nofollow noopener" target="_blank">LinkedIn</a> / <a href="/assets/pdf/Resume.pdf">CV</a></p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Feb 21, 2026</th> <td> Two papers <a href="https://tahakoleilat.github.io/MedCLIPSeg/"><b>MedCLIPSeg</b></a> and <a href=""><b>MedQwen</b></a> were accepted to <strong>CVPR 2026</strong>, see you in <strong>Denver, Colorado</strong>! </td> </tr> <tr> <th scope="row">Dec 24, 2025</th> <td> I received the <strong>IEEE TMI Gold-Level Distinguished Reviewer Certificate</strong> in recognition of my reviewing contributions. </td> </tr> <tr> <th scope="row">Oct 5, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2502.13693" rel="external nofollow noopener" target="_blank"><b>MedViTV2</b></a> was accepted to <b>Applied Soft Computing</b>. Congrats to all co-authors! </td> </tr> <tr> <th scope="row">Jul 30, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2506.23903" rel="external nofollow noopener" target="_blank"><b>GroundingDINO-US-SAM</b></a> was accepted to <strong>IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control (TUFFC)</strong>. Congrats to all co-authors! </td> </tr> <tr> <th scope="row">Jul 22, 2025</th> <td> Our paper <a href="https://tahakoleilat.github.io/MedCLIP-SAMv2/"><b>MedCLIP-SAMv2</b></a> was accepted to <strong>Medical Image Analysis (MedIA)</strong>. Congrats to all co-authors! </td> </tr> <tr> <th scope="row">Jul 10, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2507.18082" rel="external nofollow noopener" target="_blank"><b>TextSAM-EUS</b></a> was accepted to the <b>CVAMD Workshop (ICCV 2025)</b>, see you in <b>Honolulu, Hawaii</b>! </td> </tr> <tr> <th scope="row">Apr 30, 2025</th> <td> I was awarded the <strong>FRQNT Doctoral Training Scholarship</strong>, ranked <strong>2nd province-wide</strong>, supporting my research on Vision–Language Models for healthcare. </td> </tr> <tr> <th scope="row">Mar 15, 2025</th> <td> I am serving as a <strong>reviewer for MICCAI 2025</strong>. </td> </tr> </table> </div> </div> <p><a href="/news/">View all news</a></p> <h2><a href="/publications/" style="color: inherit;">Selected publications</a></h2> <p>* denotes joint first authors</p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/medclipseg_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/medclipseg_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/medclipseg_overview-1400.webp"></source> <img src="/assets/img/publication_preview/medclipseg_overview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="medclipseg_overview.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koleilat2026medclipseg" class="col-sm-8"> <div class="title">MedCLIPSeg: Probabilistic Vision–Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation</div> <div class="author"> Taha Koleilat, Hojat Asgariandehkordi, Omid Nejati Manzari, Berardino Barile, Yiming Xiao*, and Hassan Rivaz*</div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2602.20423" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/MedCLIPSeg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> <a href="https://tahakoleilat.github.io/MedCLIPSeg/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">koleilat2026medclipseg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MedCLIPSeg: Probabilistic Vision–Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koleilat, Taha and Asgariandehkordi, Hojat and Nejati Manzari, Omid and Barile, Berardino and Xiao*, Yiming and Rivaz*, Hassan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/biomedcoop_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/biomedcoop_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/biomedcoop_overview-1400.webp"></source> <img src="/assets/img/publication_preview/biomedcoop_overview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="biomedcoop_overview.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koleilat2025biomedcoop" class="col-sm-8"> <div class="title">Biomedcoop: Learning to prompt for biomedical vision-language models</div> <div class="author"> Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao</div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2411.15232" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/BiomedCoOp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> <a href="https://tahakoleilat.github.io/BiomedCoOp/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">koleilat2025biomedcoop</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Biomedcoop: Learning to prompt for biomedical vision-language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koleilat, Taha and Asgariandehkordi, Hojat and Rivaz, Hassan and Xiao, Yiming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{14766--14776}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/medclipsamv2_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/medclipsamv2_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/medclipsamv2_overview-1400.webp"></source> <img src="/assets/img/publication_preview/medclipsamv2_overview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="medclipsamv2_overview.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koleilat2025medclip" class="col-sm-8"> <div class="title">Medclip-samv2: Towards universal text-driven medical image segmentation</div> <div class="author"> Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao</div> <div class="periodical"> <em>Medical Image Analysis</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2409.19483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/MedCLIP-SAMv2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> <a href="https://tahakoleilat.github.io/MedCLIP-SAMv2/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Segmentation of anatomical structures and pathologies in medical images is essential for modern disease diagnosis, clinical research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. Recently, foundation models like CLIP and Segment-Anything-Model (SAM) have paved the way for interactive and universal image segmentation. In this work, we introduce <b>MedCLIP-SAMv2</b>, a framework that integrates <b>BiomedCLIP</b> and <b>SAM</b> to perform <b>text-driven medical image segmentation</b> in <b>zero-shot</b> and <b>weakly supervised</b> settings. The approach fine-tunes BiomedCLIP with a new <b>DHN-NCE</b> loss and leverages <b>M2IB</b> to create visual prompts for SAM; we also explore uncertainty-aware refinement via checkpoint ensembling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">koleilat2025medclip</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Medclip-samv2: Towards universal text-driven medical image segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koleilat, Taha and Asgariandehkordi, Hojat and Rivaz, Hassan and Xiao, Yiming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Medical Image Analysis}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103749}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/textsam_eus-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/textsam_eus-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/textsam_eus-1400.webp"></source> <img src="/assets/img/publication_preview/textsam_eus.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="textsam_eus.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="spiegler2025textsam" class="col-sm-8"> <div class="title">Textsam-eus: Text prompt learning for sam to accurately segment pancreatic tumor in endoscopic ultrasound</div> <div class="author"> Pascal Spiegler*, Taha Koleilat*, Arash Harirpoush, Corey S Miller, Hassan Rivaz, Marta Kersten-Oertel, and Yiming Xiao</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2507.18082" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/TextSAM-EUS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM’s architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">spiegler2025textsam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Textsam-eus: Text prompt learning for sam to accurately segment pancreatic tumor in endoscopic ultrasound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Spiegler*, Pascal and Koleilat*, Taha and Harirpoush, Arash and Miller, Corey S and Rivaz, Hassan and Kersten-Oertel, Marta and Xiao, Yiming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{948--957}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/CLIP-SVD-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/CLIP-SVD-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/CLIP-SVD-1400.webp"></source> <img src="/assets/img/publication_preview/CLIP-SVD.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="CLIP-SVD.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koleilat2025singular" class="col-sm-8"> <div class="title">Singular Value Few-shot Adaptation of Vision-Language Models</div> <div class="author"> Taha Koleilat, Hassan Rivaz, and Yiming Xiao</div> <div class="periodical"> <em>arXiv preprint arXiv:2509.03740</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2509.03740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/CLIP-SVD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> <a href="https://tahakoleilat.github.io/CLIP-SVD/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present <strong>CLIP-SVD</strong>, a novel <em>multi-modal</em> and <em>parameter-efficient</em> adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only <strong>0.04%</strong> of the model’s total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to enable interpretability of <strong>CLIP-SVD</strong>.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">koleilat2025singular</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Singular Value Few-shot Adaptation of Vision-Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koleilat, Taha and Rivaz, Hassan and Xiao, Yiming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2509.03740}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/medclipsam_framework-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/medclipsam_framework-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/medclipsam_framework-1400.webp"></source> <img src="/assets/img/publication_preview/medclipsam_framework.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="medclipsam_framework.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koleilat2024medclip" class="col-sm-8"> <div class="title">Medclip-sam: Bridging text and image towards universal medical image segmentation</div> <div class="author"> Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao</div> <div class="periodical"> <em>In International conference on medical image computing and computer-assisted intervention</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2403.20253" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://github.com/HealthX-Lab/MedCLIP-SAM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Code </a> <a href="https://tahakoleilat.github.io/MedCLIP-SAM/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. While deep learning-based methods have achieved strong performance, they often lack data efficiency, generalizability, and interactability. In this work, we propose <b>MedCLIP-SAM</b>, a novel framework that bridges vision–language models and segmentation foundation models to enable <b>text-driven universal medical image segmentation</b>. MedCLIP-SAM integrates BiomedCLIP fine-tuned with a <b>Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE)</b> loss, <b>gScoreCAM</b>-based saliency generation, CRF post-processing, and <b>Segment Anything Model (SAM)</b> refinement. The framework supports both <b>zero-shot</b> and <b>weakly supervised</b> segmentation and is validated across breast ultrasound, brain MRI, and chest X-ray datasets, demonstrating strong accuracy and generalization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">koleilat2024medclip</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Medclip-sam: Bridging text and image towards universal medical image segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koleilat, Taha and Asgariandehkordi, Hojat and Rivaz, Hassan and Xiao, Yiming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International conference on medical image computing and computer-assisted intervention}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{643--653}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%74%61%68%61%6B%6F%6C%65%69%6C%61%74@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=chB2OjUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/TahaKoleilat" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/tahakoleilat" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Taha Koleilat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>