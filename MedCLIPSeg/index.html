<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="MedCLIPSeg: Probabilistic Vision–Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation" />
  <meta name="keywords" content="MedCLIPSeg, CLIP, Vision-Language, Medical Image Segmentation, Uncertainty, Domain Generalization" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>MedCLIPSeg</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="icon" href="./static/images/favicon.svg" />

</head>

<body>

<!-- ================= HERO ================= -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        MedCLIPSeg: Probabilistic Vision–Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation
      </h1>

      <div class="is-size-5 publication-authors">
        Taha Koleilat · Hojat Asgariandehkordi · Omid Nejati Manzari · Berardino Barile ·
        Yiming Xiao<sup>†</sup> · Hassan Rivaz<sup>†</sup>
      </div>

      <p class="is-size-7 has-text-grey">
        <sup>†</sup>Co-senior authors
      </p>

      <div class="is-size-6 publication-authors">
        <a href="https://users.encs.concordia.ca/~impact/" target="_blank" rel="noreferrer">IMPACT Lab</a>
        &nbsp;·&nbsp;
        <a href="http://www.healthx-lab.ca/" target="_blank" rel="noreferrer">Health-X Lab</a>
      </div>

      <div class="buttons is-centered mt-4">
        <!-- Update arXiv if/when final ID is available -->
        <a class="button is-dark is-rounded" href="https://arxiv.org/abs/2602.20423" target="_blank" rel="noreferrer">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>

        <!-- Update GitHub repo link as needed -->
        <a class="button is-dark is-rounded" href="https://github.com/HealthX-Lab/MedCLIPSeg" target="_blank" rel="noreferrer">
          <span class="icon">
            <img src="./static/icons/github.webp" alt="GitHub" style="height:18px;" />
          </span>
          <span>Code</span>
        </a>

        <a class="button is-dark is-rounded" href="https://huggingface.co/TahaKoleilat/MedCLIPSeg" target="_blank" rel="noreferrer">
          <span class="icon">
            <img src="./static/icons/models.png" alt="Models" style="height:18px;" />
          </span>
          <span>Models</span>
        </a>

        <a class="button is-dark is-rounded" href="https://huggingface.co/datasets/TahaKoleilat/MedCLIPSeg" target="_blank" rel="noreferrer">
          <span class="icon">
            <img src="./static/icons/datasets.png" alt="Datasets" style="height:18px;" />
          </span>
          <span>Datasets</span>
        </a>

        <a class="button is-dark is-rounded" href="#BibTeX">
          <span class="icon">
            <img src="./static/icons/bibtex.png" alt="BibTeX" style="height:18px;" />
          </span>
          <span>BibTeX</span>
        </a>
      </div>

      <p class="is-size-6 has-text-grey mt-2">
        Accepted to <i>CVPR</i> (2026).
      </p>
    </div>
  </div>
</section>

<!-- ================= TEASER ================= -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Put your main overview figure here -->
      <img src="./static/images/overview.png" class="teaser-image" alt="MedCLIPSeg overview figure" />
      <h2 class="subtitle has-text-centered">
      Probabilistic cross-modal fusion for CLIP-based medical image segmentation, modeling visual–text representations as distributions to improve robustness and uncertainty calibration across in-distribution and out-of-distribution data.
    </h2>
    </div>
  </div>
</section>

<!-- ================= ABSTRACT ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features,
        and domain shifts. While vision–language models such as <b>CLIP</b> offer strong cross-modal representations, their potential
        for dense, text-guided medical image segmentation remains underexplored. We present <b>MedCLIPSeg</b>, a novel framework
        that adapts CLIP for <b>robust, data-efficient, and uncertainty-aware</b> medical image segmentation. Our approach leverages
        <b>patch-level CLIP embeddings</b> through <b>probabilistic cross-modal attention</b>, enabling bidirectional interaction between
        image and text tokens and explicit modeling of predictive uncertainty. Together with a <b>soft patch-level contrastive loss</b>
        that encourages nuanced semantic learning across diverse textual prompts, MedCLIPSeg improves data efficiency and domain
        generalizability. Extensive experiments across <b>16 datasets</b>, spanning <b>five imaging modalities</b> and <b>six organs</b>,
        demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable
        uncertainty maps that highlight the local reliability of segmentation results.
      </p>
    </div>
  </div>
</section>

<!-- ================= METHOD ================= -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>

    <div class="content has-text-justified">
      <ul>
        <li><b>Bidirectional Vision–Language Fusion:</b> Representation-level adapters enable two-way interaction between image patches and text tokens while keeping CLIP encoders frozen.</li>
        <li><b>Probabilistic Cross-Modal Attention (PVL):</b> Variational <i>Key</i> and <i>Value</i> distributions model uncertainty; confidence-weighted attention downweights unreliable tokens.</li>
        <li><b>Pixel-Level Uncertainty Estimation:</b> Monte Carlo sampling of Value distributions yields mean masks and entropy-based uncertainty maps.</li>
        <li><b>Soft Patch-Level Contrastive Loss:</b> Encourages nuanced alignment across diverse prompts and improves generalization under limited supervision.</li>
      </ul>

    </div>

    <div class="paper-figure">
      <figure class="image">
        <img src="./static/images/MedCLIPSeg.png" alt="Overall architecture of MedCLIPSeg" />
      </figure>
      <p class="has-text-centered is-size-7 mt-2">
        Overall architecture of MedCLIPSeg integrating probabilistic vision–language fusion into a CLIP-based segmentation pipeline.
      </p>
    </div>

    <div class="paper-figure mt-5">
      <figure class="image">
        <img src="./static/images/MedCLIPSeg_PVL.png" alt="PVL adapter schematic" />
      </figure>
      <p class="has-text-centered is-size-7 mt-2">
        Probabilistic Vision–Language (PVL) adapters for confidence-weighted, bidirectional cross-modal interaction.
      </p>
    </div>
  </div>
</section>

<!-- ================= RESULTS ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <div class="content has-text-justified">
      <p>
        We evaluate (i) <b>data efficiency</b> by training with 10% / 25% / 50% / 100% of available data, and
        (ii) <b>domain generalization</b> by training on an in-distribution source dataset and testing on unseen target datasets without adaptation.
        Metrics: <b>DSC</b> and <b>NSD</b>.
      </p>
    </div>

    <!-- ===== Data Efficiency (Table 1) ===== -->
    <h3 class="title is-4">Data-Efficiency Evaluation</h3>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth tight-table">
        <thead>
          <tr>
            <th rowspan="2">Method</th>
            <th colspan="2">10% Data</th>
            <th colspan="2">25% Data</th>
            <th colspan="2">50% Data</th>
            <th colspan="2">100% Data</th>
          </tr>
          <tr>
            <th>DSC ↑</th><th>NSD ↑</th>
            <th>DSC ↑</th><th>NSD ↑</th>
            <th>DSC ↑</th><th>NSD ↑</th>
            <th>DSC ↑</th><th>NSD ↑</th>
          </tr>
        </thead>
        <tbody>
          <tr class="has-background-light"><td colspan="9"><b>Unimodal Approaches</b></td></tr>
          <tr><td>UNet</td><td>60.95</td><td>64.43</td><td>62.74</td><td>66.16</td><td>71.61</td><td>75.14</td><td>78.49</td><td>82.07</td></tr>
          <tr><td>UNet++</td><td>63.72</td><td>67.08</td><td>65.86</td><td>69.21</td><td>73.15</td><td>76.31</td><td>78.44</td><td>81.79</td></tr>
          <tr><td>DeepLabv3</td><td>61.32</td><td>64.84</td><td>65.39</td><td>69.10</td><td>68.58</td><td>72.57</td><td>73.28</td><td>77.42</td></tr>
          <tr><td>Attention U-Net</td><td>62.78</td><td>66.25</td><td>64.97</td><td>68.53</td><td>71.34</td><td>74.96</td><td>76.30</td><td>79.77</td></tr>
          <tr><td>nnU-Net</td><td>73.45</td><td>77.37</td><td>76.73</td><td>80.66</td><td>78.86</td><td>82.68</td><td>81.40</td><td>85.08</td></tr>
          <tr><td>Swin-UNet</td><td>53.04</td><td>57.91</td><td>54.69</td><td>59.24</td><td>55.89</td><td>61.25</td><td>65.03</td><td>69.32</td></tr>
          <tr><td>TransUNet</td><td>52.69</td><td>56.38</td><td>55.25</td><td>58.95</td><td>55.22</td><td>59.30</td><td>67.22</td><td>71.15</td></tr>

          <tr class="has-background-light"><td colspan="9"><b>Generic Text-driven Approaches</b></td></tr>
          <tr><td>LViT</td><td>66.51</td><td>68.80</td><td>75.66</td><td>78.12</td><td>78.88</td><td>81.34</td><td>83.35</td><td>85.89</td></tr>
          <tr><td>Ariadne’s Thread</td><td>61.34</td><td>62.75</td><td>63.09</td><td>64.51</td><td>65.65</td><td>66.92</td><td>70.07</td><td>71.49</td></tr>

          <tr class="has-background-light"><td colspan="9"><b>CLIP-based Approaches</b></td></tr>
          <tr><td>EoMT-CLIP</td><td>74.07</td><td>77.41</td><td>76.29</td><td>79.84</td><td>79.19</td><td>82.78</td><td>82.93</td><td>86.35</td></tr>
          <tr><td>CLIPSeg</td><td>74.66</td><td>77.75</td><td>78.31</td><td>81.34</td><td>79.63</td><td>82.58</td><td>84.87</td><td>87.74</td></tr>
          <tr><td>DenseCLIP</td><td>67.84</td><td>70.33</td><td>70.23</td><td>72.70</td><td>72.09</td><td>74.45</td><td>74.19</td><td>76.89</td></tr>
          <tr><td>ZegCLIP</td><td>61.25</td><td>63.72</td><td>72.46</td><td>75.01</td><td>76.21</td><td>78.80</td><td>78.98</td><td>81.69</td></tr>
          <tr><td>SAN</td><td>74.13</td><td>76.97</td><td>76.13</td><td>78.91</td><td>78.80</td><td>81.52</td><td>81.62</td><td>84.35</td></tr>
          <tr><td>MaPLe</td><td>66.27</td><td>68.75</td><td>71.53</td><td>73.95</td><td>74.60</td><td>77.12</td><td>74.60</td><td>77.10</td></tr>
          <tr><td>MaPLe + Decoder</td><td>74.81</td><td>77.90</td><td>79.64</td><td>82.60</td><td>82.81</td><td>85.80</td><td>84.94</td><td>87.91</td></tr>
          <tr><td>VLSM-Adapter</td><td>74.47</td><td>77.50</td><td>77.63</td><td>80.53</td><td>80.83</td><td>83.77</td><td>83.85</td><td>86.72</td></tr>
          <tr><td>CausalCLIPSeg</td><td>71.19</td><td>73.74</td><td>75.42</td><td>78.00</td><td>78.60</td><td>81.22</td><td>81.34</td><td>84.20</td></tr>
          <tr><td>CAT-Seg</td><td>78.76</td><td>81.50</td><td>81.12</td><td>83.92</td><td>83.32</td><td>85.61</td><td>85.90</td><td>88.31</td></tr>

          <tr class="is-success-light">
            <td><b>MedCLIPSeg (Ours)</b></td>
            <td><b>81.10</b></td><td><b>83.94</b></td>
            <td><b>85.08</b></td><td><b>87.85</b></td>
            <td><b>87.18</b></td><td><b>89.95</b></td>
            <td><b>88.66</b></td><td><b>91.35</b></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- ===== Domain Generalization (Table 2) ===== -->
    <h3 class="title is-4 mt-6">Domain Generalization (DSC %)</h3>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth tight-table">
        <thead>
          <tr>
            <th rowspan="2">Method</th>
            <th colspan="4">Breast Ultrasound</th>
            <th colspan="4">Polyp Endoscopy</th>
            <th colspan="2">Brain MRI</th>
            <th colspan="2">Skin Dermatoscopy</th>
          </tr>
          <tr>
            <th>BUSI</th><th>BUSBRA</th><th>BUSUC</th><th>BUID</th>
            <th>UDIAT</th><th>Kvasir-SEG</th><th>ColonDB</th><th>ClinicDB</th>
            <th>BTMRI</th><th>BRISC</th>
            <th>ISIC</th><th>UWaterloo</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>LViT</td><td>75.32</td><td>59.41</td><td>67.95</td><td>53.51</td><td>65.60</td><td>85.29</td><td>60.01</td><td>75.27</td><td>81.41</td><td>71.86</td><td>91.21</td><td>58.87</td></tr>
          <tr><td>CLIPSeg</td><td>80.95</td><td>63.66</td><td>75.03</td><td>68.43</td><td>56.67</td><td>81.98</td><td>59.93</td><td>71.49</td><td>86.33</td><td>77.61</td><td>90.55</td><td>80.19</td></tr>
          <tr><td>DenseCLIP</td><td>71.85</td><td>53.34</td><td>70.97</td><td>63.53</td><td>54.93</td><td>79.32</td><td>56.38</td><td>68.08</td><td>70.30</td><td>34.12</td><td>89.29</td><td>53.39</td></tr>
          <tr><td>ZegCLIP</td><td>72.08</td><td>61.08</td><td>73.57</td><td>71.75</td><td>52.41</td><td>78.46</td><td>53.46</td><td>69.75</td><td>76.65</td><td>66.31</td><td>81.45</td><td>38.60</td></tr>
          <tr><td>SAN</td><td>77.99</td><td>64.37</td><td>74.15</td><td>58.13</td><td>61.98</td><td>83.16</td><td>61.82</td><td>74.46</td><td>85.27</td><td>71.60</td><td>91.39</td><td>82.51</td></tr>
          <tr><td>MaPLe</td><td>66.37</td><td>50.08</td><td>71.52</td><td>70.77</td><td>57.81</td><td>76.12</td><td>48.09</td><td>59.64</td><td>75.40</td><td>45.19</td><td>88.31</td><td>69.12</td></tr>
          <tr><td>MaPLe + Decoder</td><td>80.49</td><td>55.89</td><td>64.96</td><td>60.66</td><td>59.44</td><td>83.46</td><td>61.53</td><td>71.20</td><td>85.08</td><td>71.46</td><td>90.10</td><td>81.83</td></tr>
          <tr><td>VLSM-Adapter</td><td>80.90</td><td>68.48</td><td>82.37</td><td>75.26</td><td>69.16</td><td>85.89</td><td>63.51</td><td>76.09</td><td>85.03</td><td>68.92</td><td>91.30</td><td>82.17</td></tr>
          <tr><td>CausalCLIPSeg</td><td>76.11</td><td>55.87</td><td>69.12</td><td>64.49</td><td>48.90</td><td>78.77</td><td>41.65</td><td>57.54</td><td>81.71</td><td>53.96</td><td>89.47</td><td>48.73</td></tr>
          <tr><td>CAT-Seg</td><td>81.83</td><td>70.94</td><td>81.48</td><td>73.37</td><td>70.30</td><td>86.43</td><td>68.49</td><td>70.35</td><td>84.86</td><td>76.28</td><td>91.27</td><td>82.02</td></tr>

          <tr class="is-success-light">
            <td><b>MedCLIPSeg (Ours)</b></td>
            <td><b>85.72</b></td><td><b>75.06</b></td><td><b>84.37</b></td><td><b>78.99</b></td>
            <td><b>74.64</b></td><td><b>90.15</b></td><td><b>71.90</b></td><td><b>80.80</b></td>
            <td><b>88.03</b></td><td><b>80.92</b></td>
            <td><b>92.54</b></td><td><b>83.53</b></td>
          </tr>
        </tbody>
      </table>
    </div>

  </div>
</section>

<!-- ================= UNCERTAINTY VISUALS ================= -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Segmentation &amp; Uncertainty Visualization</h2>

    <div class="content has-text-justified">
      <p>
        MedCLIPSeg produces both a segmentation mask and a dense uncertainty map.
        Uncertainty tends to peak along ambiguous boundaries and challenging regions, and remains consistent across in-distribution and
        out-of-distribution samples—supporting interpretability and reliability review.
      </p>
    </div>

    <figure class="image paper-figure">
      <!-- Use the composite visualization figure here -->
      <img src="./static/images/MedCLIPSeg_Seg.jpg" alt="Segmentation and uncertainty visualization" />
      <figcaption class="has-text-centered is-size-7 mt-2">
        Example predictions with uncertainty maps. ID datasets are in blue while OOD datasets are in red.
      </figcaption>
    </figure>
  </div>
</section>

    <!-- ================= ABLATION STUDIES ================= -->
    <!-- ================= ABLATION STUDIES ================= -->
<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3 has-text-centered">Ablation Studies</h2>

    <div class="content has-text-justified">
      <p>
        We analyze the contribution of MedCLIPSeg’s components and design choices, including
        PVL adapters, gating, probabilistic attention, bidirectional interaction, contrastive loss,
        prompt style, and CLIP backbone selection.
      </p>
    </div>

    <!-- ===== Table 3: Key component ablations ===== -->
    <h4 class="title is-5 mt-5">Effectiveness of Key Design Components</h4>

    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth tight-table">
        <thead>
          <tr>
            <th>Method</th>
            <th>ID DSC (%)</th>
            <th>OOD DSC (%)</th>
            <th>HM DSC (%)</th>
          </tr>
        </thead>

        <tbody>

          <tr class="is-success-light">
            <td><b>MedCLIPSeg (Ours)</b></td>
            <td><b>89.11</b></td>
            <td><b>79.02</b></td>
            <td><b>83.76</b></td>
          </tr>

          <tr class="has-background-light">
            <td colspan="4"><b>Probabilistic Vision–Language Adapters</b></td>
          </tr>

          <tr>
            <td>w/o PVL Adapters</td>
            <td>81.23 (−7.88)<span class="red-arrow">↓</span></td>
            <td>55.23 (−23.79)<span class="red-arrow">↓</span></td>
            <td>65.75 (−18.01)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>w/o Gating</td>
            <td>87.55 (−1.56)<span class="red-arrow">↓</span></td>
            <td>76.79 (−2.23)<span class="red-arrow">↓</span></td>
            <td>81.82 (−1.94)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>w/o AttnPVL</td>
            <td>86.21 (−2.90)<span class="red-arrow">↓</span></td>
            <td>74.13 (−4.89)<span class="red-arrow">↓</span></td>
            <td>79.71 (−4.05)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>Deterministic MedCLIPSeg</td>
            <td>87.68 (−1.43)<span class="red-arrow">↓</span></td>
            <td>63.12 (−15.90)<span class="red-arrow">↓</span></td>
            <td>73.40 (−10.36)<span class="red-arrow">↓</span></td>
          </tr>

          <tr class="has-background-light">
            <td colspan="4"><b>Bidirectional Multimodal Interaction</b></td>
          </tr>

          <tr>
            <td>w/o Visual Adaptation</td>
            <td>81.50 (−7.61)<span class="red-arrow">↓</span></td>
            <td>64.40 (−14.62)<span class="red-arrow">↓</span></td>
            <td>71.95 (−11.81)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>w/o Textual Adaptation</td>
            <td>88.83 (−0.28)<span class="red-arrow">↓</span></td>
            <td>76.40 (−2.62)<span class="red-arrow">↓</span></td>
            <td>82.15 (−1.61)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>w/o Bidirectional Interaction</td>
            <td>88.71 (−0.40)<span class="red-arrow">↓</span></td>
            <td>77.71 (−1.31)<span class="red-arrow">↓</span></td>
            <td>82.85 (−0.91)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>Unimodal MedCLIPSeg</td>
            <td>86.53 (−2.58)<span class="red-arrow">↓</span></td>
            <td>73.49 (−5.53)<span class="red-arrow">↓</span></td>
            <td>79.48 (−4.28)<span class="red-arrow">↓</span></td>
          </tr>

          <tr class="has-background-light">
            <td colspan="4"><b>Contrastive Loss</b></td>
          </tr>

          <tr>
            <td>w/o SoftCon Loss</td>
            <td>87.24 (−1.87)<span class="red-arrow">↓</span></td>
            <td>77.08 (−1.94)<span class="red-arrow">↓</span></td>
            <td>81.84 (−1.92)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>Hard Targets</td>
            <td>88.34 (−0.77)<span class="red-arrow">↓</span></td>
            <td>77.64 (−1.38)<span class="red-arrow">↓</span></td>
            <td>82.65 (−1.11)<span class="red-arrow">↓</span></td>
          </tr>

          <tr>
            <td>Attention-pooled SoftCon Loss</td>
            <td>88.73 (−0.38)<span class="red-arrow">↓</span></td>
            <td>75.60 (−3.42)<span class="red-arrow">↓</span></td>
            <td>81.64 (−2.12)<span class="red-arrow">↓</span></td>
          </tr>

        </tbody>
      </table>
    </div>


    <!-- ===== Figure 5 ===== -->
    <h4 class="title is-5 mt-5">Layer-wise PVL Intervention and Confidence Weight (&beta;)</h4>

    <div class="paper-figure">
      <figure class="image">
        <img src="./static/images/layers_beta_plot.png"
             alt="Layer-wise interventions and confidence weight (beta) ablations">
      </figure>

      <p class="has-text-centered is-size-7 mt-2">
        Layer-wise PVL adapter interventions and confidence weight (&beta;) ablations averaged on ID and OOD data.
        Deeper interventions improve performance up to ~Layer 10, and &beta; = 2.35 yields the best harmonic mean.
      </p>
    </div>


    <!-- ===== Table 4 ===== -->
    <h4 class="title is-5 mt-6">Effect of Text Prompt Design</h4>

    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth tight-table">

        <thead>
          <tr>
            <th>Text Prompt Style</th>
            <th>ID DSC (%)</th>
            <th>OOD DSC (%)</th>
            <th>HM DSC (%)</th>
          </tr>
        </thead>

        <tbody>
          <tr><td>Contradictory</td><td>68.60</td><td>63.21</td><td>65.79</td></tr>
          <tr><td>Missing Location</td><td>86.98</td><td>77.75</td><td>82.11</td></tr>
          <tr><td>Overdescriptive</td><td>82.93</td><td>74.49</td><td>78.48</td></tr>
          <tr><td>Underdescriptive</td><td>66.91</td><td>49.38</td><td>56.82</td></tr>

          <tr class="is-success-light">
            <td><b>Original</b></td>
            <td><b>89.11</b></td>
            <td><b>79.02</b></td>
            <td><b>83.76</b></td>
          </tr>
        </tbody>

      </table>
    </div>


    <!-- ===== Table 5 ===== -->
    <h4 class="title is-5 mt-6">Effect of Pre-trained Vision–Language Models</h4>

    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth tight-table">

        <thead>
          <tr>
            <th>Pre-trained Model</th>
            <th>ID DSC (%)</th>
            <th>OOD DSC (%)</th>
            <th>HM DSC (%)</th>
          </tr>
        </thead>

        <tbody>
          <tr><td>CLIP</td><td>88.48</td><td>74.81</td><td>81.07</td></tr>
          <tr><td>PubMedCLIP</td><td>86.67</td><td>73.05</td><td>79.28</td></tr>
          <tr><td>BiomedCLIP</td><td>88.70</td><td>77.08</td><td>82.48</td></tr>

          <tr class="is-success-light">
            <td><b>UniMedCLIP</b></td>
            <td><b>89.11</b></td>
            <td><b>79.02</b></td>
            <td><b>83.76</b></td>
          </tr>
        </tbody>

      </table>
    </div>

  </div>
</section>

<!-- ================= BIBTEX ================= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@inproceedings{koleilat2026medclipseg,
  title     = {MedCLIPSeg: Probabilistic Vision--Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation},
  author    = {Koleilat, Taha and Asgariandehkordi, Hojat and Nejati Manzari, Omid and Barile, Berardino and Xiao, Yiming and Rivaz, Hassan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Website adapted from <a href="https://nerfies.github.io" target="_blank" rel="noreferrer">Nerfies</a>.
    </p>
  </div>
</footer>

</body>
</html>