<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Singular Value Few-shot Adaptation of Vision-Language Models">
  <meta name="keywords"
        content="CLIP-SVD, Vision-Language Models, Few-shot Learning, CLIP, BiomedCLIP, Medical Imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CLIP-SVD</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>

<body>

<!-- ================= HERO ================= -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">
        Singular Value Few-shot Adaptation of Vision-Language Models
      </h1>

      <div class="is-size-5 publication-authors">
        Taha Koleilat · Hassan Rivaz · Yiming Xiao
      </div>

      <div class="is-size-6 publication-authors">
        Concordia University · Health-X Lab · IMPACT Lab
      </div>

      <div class="buttons is-centered mt-4">
        <a class="button is-dark is-rounded"
           href="https://arxiv.org/abs/2509.03740"
           target="_blank">
          <span class="icon"><i class="ai ai-arxiv"></i></span>
          <span>arXiv</span>
        </a>

        <a class="button is-dark is-rounded"
           href="https://github.com/HealthX-Lab/CLIP-SVD"
           target="_blank">
          <span class="icon">
            <img src="./static/icons/github.webp" style="height:18px;">
          </span>
          <span>Code</span>
        </a>

        <a class="button is-dark is-rounded"
           href="https://huggingface.co/datasets/TahaKoleilat/BiomedCoOp"
           target="_blank">
          <span class="icon">
            <img src="./static/icons/datasets.png" style="height:18px;">
          </span>
          <span>Datasets</span>
        </a>

        <a class="button is-dark is-rounded"
           href="https://github.com/HealthX-Lab/CLIP-SVD/blob/main/medical_image_descriptions.txt"
           target="_blank">
          <span class="icon">
            <img src="./static/icons/corpus.png" style="height:18px;">
          </span>
          <span>Corpus</span>
        </a>

        <a class="button is-dark is-rounded" href="#BibTeX">
          <span class="icon">
            <img src="./static/icons/bibtex.png" style="height:18px;">
          </span>
          <span>BibTeX</span>
        </a>
      </div>

      <p class="is-size-7 has-text-grey mt-2">
        arXiv preprint (2025).
      </p>
    </div>
  </div>
</section>

<!-- ================= TEASER ================= -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/overview.png"
           class="teaser-image"
           alt="CLIP-SVD overview">
      <h2 class="subtitle has-text-centered">
        CLIP-SVD adapts vision–language models by fine-tuning only singular values,
        achieving state-of-the-art few-shot performance using just 0.04% of parameters.
      </h2>
    </div>
  </div>
</section>

<!-- ================= ABSTRACT ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <div class="content has-text-justified">
      <p style="text-align: justify;">
  <strong>Abstract:</strong>
  <em>
    Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
    few-shot learning capabilities across diverse applications. However, adapting
    these models to new fine-grained domains remains difficult due to reliance on
    prompt engineering and the high cost of full model fine-tuning. Existing
    adaptation approaches rely on augmented components, such as prompt tokens and
    adapter modules, which could limit adaptation quality, destabilize the model,
    and compromise the rich knowledge learned during pretraining. In this work, we
    present <strong>CLIP-SVD</strong>, a novel <em>multi-modal</em> and
    <em>parameter-efficient</em> adaptation technique that leverages Singular Value
    Decomposition (SVD) to modify the internal parameter space of CLIP without
    injecting additional modules. Specifically, we fine-tune only the singular
    values of the CLIP parameter matrices to rescale the basis vectors for domain
    adaptation while retaining the pretrained model. This design enables enhanced
    adaptation performance using only <strong>0.04%</strong> of the model's total
    parameters and better preservation of its generalization ability. CLIP-SVD
    achieves state-of-the-art classification results on 11 natural and 10 biomedical
    datasets, outperforming previous methods in both accuracy and generalization
    under few-shot settings. Additionally, we leverage a natural language-based
    approach to analyze the effectiveness and dynamics of the CLIP adaptation to
    enable interpretability of <strong>CLIP-SVD</strong>.
  </em>
</p>
    </div>
  </div>
</section>

<!-- ================= METHOD ================= -->
<section class="section is-light" id="overview">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>

    <div class="content has-text-justified">
      <ul>
        <li><b>SVD-based Adaptation:</b> Fine-tunes only singular values.</li>
        <li><b>Extreme Parameter Efficiency:</b> 0.04% trainable parameters.</li>
        <li><b>Multi-modal:</b> Applicable to CLIP and BiomedCLIP.</li>
        <li><b>Preserved Generalization:</b> Keeps pretrained bases intact.</li>
        <li><b>Interpretability:</b> Natural language analysis of singular value dynamics.</li>
      </ul>
    </div>

    <figure class="image">
      <img src="./static/images/CLIP-SVD.png" alt="CLIP-SVD method">
      <figcaption class="has-text-centered is-size-7 mt-2">
        CLIP-SVD adapts models by rescaling singular values while preserving pretrained bases.
      </figcaption>
    </figure>
  </div>
</section>

<!-- ================= RESULTS ================= -->
<!-- ================= RESULTS ================= -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <!-- ===== Natural Few-shot Evaluation ===== -->
    <h3 class="title is-4 mt-5">Natural Few-shot Evaluation</h3>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Method</th>
            <th>K = 1</th>
            <th>K = 2</th>
            <th>K = 4</th>
            <th>K = 8</th>
            <th>K = 16</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Zero-shot CLIP</td>
            <td>–</td><td>–</td><td>65.36</td><td>–</td><td>–</td>
          </tr>
          <tr>
            <td>CoOp</td>
            <td>68.09</td><td>70.13</td><td>73.59</td><td>76.45</td><td>79.01</td>
          </tr>
          <tr>
            <td>MaPLe</td>
            <td>69.27</td><td>72.58</td><td>75.37</td><td>78.89</td><td>81.79</td>
          </tr>
          <tr>
            <td>CLIP-LoRA</td>
            <td><i>72.20</i></td><td><i>75.41</i></td><td><i>77.32</i></td>
            <td><i>80.10</i></td><td><i>82.89</i></td>
          </tr>
          <tr class="has-background-success-light">
            <td><b>CLIP-SVD (Ours)</b></td>
            <td><b>73.20</b></td><td><b>76.06</b></td><td><b>78.18</b></td>
            <td><b>80.55</b></td><td><b>82.97</b></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- ===== Biomedical Few-shot Evaluation ===== -->
    <h3 class="title is-4 mt-6">Biomedical Few-shot Evaluation</h3>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Method</th>
            <th>K = 1</th>
            <th>K = 2</th>
            <th>K = 4</th>
            <th>K = 8</th>
            <th>K = 16</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Zero-shot BiomedCLIP</td>
            <td>–</td><td>–</td><td>42.38</td><td>–</td><td>–</td>
          </tr>
          <tr>
            <td>CoOp</td>
            <td>52.59</td><td>55.71</td><td>61.35</td><td>67.74</td><td>71.48</td>
          </tr>
          <tr>
            <td>BiomedCoOp</td>
            <td><b>56.87</b></td><td><i>59.32</i></td><td><i>64.34</i></td>
            <td><i>68.96</i></td><td><i>73.41</i></td>
          </tr>
          <tr class="has-background-success-light">
            <td><b>CLIP-SVD (Ours)</b></td>
            <td><i>56.35</i></td><td><b>62.63</b></td><td><b>68.02</b></td>
            <td><b>73.26</b></td><td><b>76.46</b></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- ===== Base-to-Novel Generalization ===== -->
    <h3 class="title is-4 mt-6">Base-to-Novel Generalization (Biomedical)</h3>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Method</th>
            <th>Base</th>
            <th>Novel</th>
            <th>HM</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>BiomedCLIP</td>
            <td>49.27</td><td>67.17</td><td>55.23</td>
          </tr>
          <tr>
            <td>BiomedCoOp</td>
            <td>78.60</td><td>73.90</td><td>74.04</td>
          </tr>
          <tr class="has-background-success-light">
            <td><b>CLIP-SVD (Ours)</b></td>
            <td><b>82.64</b></td><td><b>74.31</b></td><td><b>78.25</b></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- ===== Interpretability Evaluation ===== -->
    <h3 class="title is-4 mt-6">Interpretability Evaluation</h3>
    <div class="content has-text-justified">
      <p>
        CLIP-SVD enables a principled analysis of adaptation dynamics by inspecting
        ranked singular value updates and mapping them to natural language concepts.
        This facilitates interpretation of <i>which semantic dimensions are amplified
        or suppressed</i> during few-shot learning, while preserving the pretrained
        representation structure.
      </p>
    </div>

    <figure class="image mt-4">
      <img src="./static/images/interpretability_table.png"
           alt="Interpretability analysis of CLIP-SVD using singular value rankings">
      <figcaption class="has-text-centered is-size-7 mt-2">
        Interpretability analysis of CLIP-SVD. Ranked singular value updates are
        associated with natural language descriptions, revealing task-relevant
        semantic shifts while maintaining stable lower-rank representations.
      </figcaption>
    </figure>

  </div>
</section>


<!-- ================= BIBTEX ================= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@article{koleilat2025singular,
  title={Singular Value Few-shot Adaptation of Vision-Language Models},
  author={Koleilat, Taha and Rivaz, Hassan and Xiao, Yiming},
  journal={arXiv preprint arXiv:2509.03740},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Website adapted from <a href="https://nerfies.github.io">Nerfies</a>.
    </p>
  </div>
</footer>

</body>
</html>
